{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e14c1a2-4c10-449b-a808-6b78aacca373",
   "metadata": {},
   "source": [
    "# Fine Tuning LLM with DPO and LoRA\n",
    "\n",
    "Adrian P. Bustamante, Ph.D.\\\n",
    "adrianpebus@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdf8e9-88d3-4f27-8dbc-43f6a6a0da8a",
   "metadata": {},
   "source": [
    "### __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Data-Processing\">Data Processing</a></li>\n",
    "    <li><a href=\"#Creating-and-configuring-model,-tokenizer,-LoRA,-and-DPO\">Creating and configuring model, tokenizer, LoRA, and DPO </a></li>\n",
    "    <li><a href=\"#DPO-training\">DPO training</a></li>\n",
    "    <li><a href=\"#Generating-Responses\">Generating Responses</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ada881-55dd-47a2-9735-7077eda8fbb5",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37cd8d1-e088-4ae7-bb79-46fce3a23245",
   "metadata": {},
   "source": [
    "The aim of this notebook is to use Direct Preference Optimization (DPO) and Low-Rank Adaptation (LoRA) to fine tune a LLM. We use the 'trl' library and the \"ultrafeedback_binarized\" dataset from Hugging Face to fine tune the model. The dataset uses a completion criteria based on helpfulness and honesty. Therefore, the aim of the fine-tune is to get more 'helpful and honest' responses from the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dbec3-b879-455b-8361-7b04cac9846f",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0686c-0db7-4f28-a82a-445fb186b348",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53067486-9769-4c3c-a176-6131a962fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.45.2\n",
    "#!pip install trl==0.11.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbd5889-7ee3-4705-8ce5-1c92713e9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import multiprocessing\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments, GPT2Tokenizer, set_seed, GenerationConfig\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171d7b5-488f-4c4b-8b09-9b2029f96855",
   "metadata": {},
   "source": [
    "### About the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd422d6-425e-4e24-8a59-2729806d55c3",
   "metadata": {},
   "source": [
    "We use the \"ultrafeedback_binarized\" data set on Hugging Face, which is a collection of prompts and responses. See https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized, for details.\n",
    "\n",
    "Dataset description:\\\n",
    "This is a pre-processed version of the UltraFeedback dataset and was used to train Zephyr-7Β-β, a state of the art chat model at the 7B parameter scale. \\\n",
    "The original UltraFeedback dataset consists of 64k prompts, where each prompt is accompanied with four model completions from a wide variety of open and proprietary models.**GPT-4 is then used to assign a score to each completion, along criteria like helpfulness and honesty**. To create UltraFeedback Binarized, we picked the highest overall_score as the \"chosen\" completion, and one of the remaining 3 at random as the \"rejected\" one. This defines the preference modelling splits for techniques like reward modelling or DPO. We also created splits for supervised fine-tuning (SFT) that use the \"chosen\" column as the dialogues to model, along with splits that involve generation like rejection sampling or PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b5a73c-1612-45ce-ab37-a1377341831e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_prefs: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    train_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    test_prefs: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    test_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"BarraHome/ultrafeedback_binarized\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dfe7cc-060b-4014-914a-c7e14afcd917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_prefs', 'train_sft', 'test_prefs', 'test_sft', 'train_gen', 'test_gen'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8725188d-9515-4dcc-8fcb-b5743479f2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train_prefs\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c524f79c-3e13-47de-aeb7-3d23fb163ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Which animal has two hands, a hyrax or a dog?',\n",
       " 'chosen': 'Neither a hyrax nor a dog has hands. Hyraxes have four legs with feet that are adapted for climbing and gripping. Similarly, dogs also have four legs with paws. The term \"hands\" is usually reserved for primates, such as humans, which have opposable thumbs and fingers.',\n",
       " 'rejected': 'Both hyraxes and dogs have four limbs. While hyraxes - like their even-toed ungulate relatives the horses and asses - have two rear hands modified for grooming and manipulating objects, dogs have front paws and hind limbs.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train_prefs'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e72e7f71-fe84-4381-adf4-1d2ee4e1f837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Write a eulogy for a public figure who inspired you.',\n",
       " 'chosen': 'Ladies and gentlemen, family and friends, distinguished guests, we gather here today to celebrate the life and legacy of a great leader, an inspirational figure, and a true patriot – the Honorable John Adams.\\n\\nJohn Adams was a man who dedicated his life to serving his country and advocating for the rights of his fellow citizens. He was a Founding Father, a statesman, a lawyer, a diplomat, and a voracious learner who never ceased to seek knowledge and wisdom. He believed that through education and hard work, anyone could rise above their circumstances and make a positive impact on the world.\\n\\nAs a member of the Continental Congress, Adams played a critical role in shaping our nation\\'s founding documents, including the Declaration of Independence and the U.S. Constitution. He believed that government should exist to serve the needs of its people, and that every person possesses inherent rights that must be protected by the rule of law.\\n\\nAdams\\' commitment to these principles did not go unnoticed, and he was often viewed as a controversial figure during his lifetime. But he never wavered in his beliefs or his dedication to serving the public good. He was a champion of human rights, a defender of the oppressed, and a voice for the voiceless.\\n\\nPerhaps most importantly, John Adams was a man who inspired others to greatness. He had a profound impact on the lives of those around him, encouraging them to pursue their passions, stand up for what they believe in, and never give up on their dreams.\\n\\nAmong those whom he inspired was his wife Abigail, who was not only his partner in life but also his intellectual equal and trusted advisor. Together, they raised a family and built a legacy that would endure for generations to come.\\n\\nJohn Adams was a man who lived by the principles he advocated for, and he left an indelible mark on our nation\\'s history. He was a true patriot who loved his country deeply and sacrificed much to ensure that it would be a beacon of hope and freedom for generations to come.\\n\\nSo today, as we gather to remember and honor John Adams, let us be inspired by his life and his legacy. Let us strive to live up to the high standards he set, and let us continue to fight for the ideals he held so dear.\\n\\nIn closing, I would like to share a quote from John Adams that captures his spirit and his vision for our nation: \"Remember, democracy never lasts long. It soon wastes, exhausts, and murders itself. There is never a dearth of wolves wishing to snatch from the people the precious fruits of freedom. It is the common tendency of human nature.\"\\n\\nLet us honor John Adams\\' memory by continuing the struggle for democracy, for freedom, and for justice. May his legacy live on, inspiring future generations to carry forth the torch of liberty and uphold the ideals that he fought so hard to defend.\\n\\nRest in peace, John Adams, and may your spirit continue to guide and inspire us always.',\n",
       " 'rejected': 'We are gathered here today to celebrate the life of Stephen Hawking, a man who inspired millions around the world with his brilliance, his courage, and his determination.\\n\\nStephen Hawking was born in Oxford, England, in 1942. He was a brilliant student, and at the age of 17, he went to study physics at University College, Oxford. After graduating, he went on to study at Trinity Hall, Cambridge, where he earned his Ph.D. in cosmology.\\n\\nIn 1963, at the age of 21, Stephen Hawking was diagnosed with amyotrophic lateral sclerosis, or ALS, a disease that gradually paralyzes the body. At the time, doctors gave him only a few years to live. But Stephen Hawking defied the odds. He went on to live a full and productive life, making groundbreaking discoveries in physics and cosmology.\\n\\nIn 1974, Stephen Hawking published his landmark book, \"A Brief History of Time.\" The book became a bestseller, and it helped to popularize the subject of cosmology. Stephen Hawking went on to write several other books, including \"The Universe in a Nutshell\" and \"The Grand Design.\"\\n\\nStephen Hawking was also a popular lecturer. He gave talks all over the world, and he appeared on many television shows. He used his platform to talk about science and to raise awareness about ALS.\\n\\nStephen Hawking was a brilliant scientist and a gifted communicator. He was a true inspiration to everyone who knew him. He will be missed by all who had the privilege of knowing him.\\n\\nBut Stephen Hawking was more than just a brilliant scientist. He was also a courageous man who never gave up. Even as his body was slowly paralyzed, he continued to work and to make new discoveries. He inspired millions of people around the world with his example of courage and determination.\\n\\nStephen Hawking was a truly remarkable man. He will be missed by all who knew him. But his legacy will live on. His work in physics and cosmology will continue to inspire and amaze us for generations to come.\\n\\nThank you.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train_prefs'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f390f9b-2ee6-4382-96ec-26a18efaefe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3349.70 examples/s]\n",
      "Map (num_proc=8): 100%|██████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3306.62 examples/s]\n",
      "Map (num_proc=8): 100%|██████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3638.92 examples/s]\n",
      "Map (num_proc=8): 100%|██████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3716.06 examples/s]\n",
      "Map (num_proc=8): 100%|██████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3713.88 examples/s]\n",
      "Map (num_proc=8): 100%|██████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3574.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#we reduce the volume of data (due to resource limitations) by selecting the first 500 entries \n",
    "#examples from each split of the dataset\n",
    "for key in ds:\n",
    "    cnt=500\n",
    "    ds[key] = ds[key].select(range(cnt))\n",
    "\n",
    "# Define a function to process the data\n",
    "## we only keep prompt, chosen, and rejected\n",
    "def process(row):\n",
    "    # delete unwanted columns\n",
    "    del row[\"prompt_id\"]\n",
    "    del row[\"messages\"]\n",
    "    del row[\"score_chosen\"]\n",
    "    del row[\"score_rejected\"]\n",
    "    # retrieve the actual response text\n",
    "    row[\"chosen\"] = row[\"chosen\"][-1][\"content\"]\n",
    "    row[\"rejected\"] = row[\"rejected\"][-1][\"content\"]\n",
    "\n",
    "    return row\n",
    "\n",
    "# Apply the data processing function to the dataset\n",
    "ds = ds.map(\n",
    "    process,\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_dataset = ds['train_prefs']\n",
    "eval_dataset = ds['test_prefs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49eb30a-1f61-48a6-a24f-e7af4a9df451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 500\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab84356-a0d6-4f21-a14c-87ffeb2f888a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'how can i develop a habit of drawing daily',\n",
       " 'chosen': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
       " 'rejected': \"One way to develop a habit of drawing daily is to allocate a specific time interval for drawing each day, whether it's early in the morning or before going to bed at night. You can also find inspiration or motivation to draw by joining drawing communities, following artists on social media, or going out into nature and sketching what you see. Additionally, practicing drawing every day can help you improve your skills over time.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87073c-5ad7-46ea-ad53-7a20665ffd8d",
   "metadata": {},
   "source": [
    "## Creating and configuring model, tokenizer, LoRA, and DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f1c6b-f691-41ed-aaf0-51710bc0ebd2",
   "metadata": {},
   "source": [
    "#### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab35232-c830-4b6c-a7ff-bac96471c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load a reference model \n",
    "model_ref = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the pad token to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set the padding side to \"right\" to fix the overflow issue with FP16 training\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Disable the use of the cache during the model's forward pass\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45c0e724-9d4d-4402-a9c0-1e3e73846566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764b90c-120a-4703-8a2c-6719f2a05624",
   "metadata": {},
   "source": [
    "#### LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74ac3267-7d49-4365-aa3d-b0cc681c172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT (Parameter-Efficient Finetuning) configuration\n",
    "peft_lora_config = LoraConfig(\n",
    "        # The rank of the low-rank adaptation weights\n",
    "        r=4,\n",
    "        # The target modules to apply the low-rank adaptation to (see model architecture)\n",
    "        target_modules=['c_proj','c_attn'],\n",
    "        # The task type for the low-rank adaptation\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # The scaling factor for the low-rank adaptation weights\n",
    "        lora_alpha=8,\n",
    "        # The dropout probability for the low-rank adaptation weights\n",
    "        lora_dropout=0.1,\n",
    "        # The bias mode for the low-rank adaptation\n",
    "        bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597988a4-a0c1-4a40-8ee2-76f0128edd9a",
   "metadata": {},
   "source": [
    "#### DPO configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47b1b33f-ee91-4533-baa0-9f6591e44196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO configuration\n",
    "#from peft import get_peft_model\n",
    "training_args = DPOConfig(\n",
    "    # The beta parameter for the DPO loss function (kullback-leibner divergence)\n",
    "    #beta is the temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5 . \n",
    "    beta=0.1,\n",
    "    # The output directory for the training\n",
    "    output_dir=\"dpo\",\n",
    "    # The number of training epochs\n",
    "    num_train_epochs=10, ##small, because we are using cpu\n",
    "    # The batch size per device during training\n",
    "    per_device_train_batch_size=1,\n",
    "    # The batch size per device during evaluation\n",
    "    per_device_eval_batch_size=1,\n",
    "    # Whether to remove unused columns from the dataset\n",
    "    remove_unused_columns=False,\n",
    "    # The number of steps between logging training progress\n",
    "    logging_steps=100, #10,\n",
    "    # The number of gradient accumulation steps\n",
    "    gradient_accumulation_steps=1,\n",
    "    # The learning rate for the optimization\n",
    "    learning_rate=1e-4,\n",
    "    # The evaluation strategy (e.g., after each step or epoch)\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    # The number of warmup steps for the learning rate scheduler\n",
    "    warmup_steps=2,\n",
    "    # Whether to use 16-bit (float16) precision\n",
    "    fp16=False,\n",
    "    # The number of steps between saving checkpoints\n",
    "    save_steps=500,#500,\n",
    "    # The maximum number of checkpoints to keep\n",
    "    #save_total_limit=2,\n",
    "    # The reporting backend to use (set to 'none' to disable, you can also report to wandb or tensorboard)\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe5cac-351c-4b13-8d24-1c011fb6853e",
   "metadata": {},
   "source": [
    "## DPO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4693bd2-9953-4ff1-a0c4-0d61ad7185a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|███████████████████████████████████████████████| 500/500 [00:02<00:00, 213.90 examples/s]\n",
      "Tokenizing eval dataset: 100%|████████████████████████████████████████████████| 500/500 [00:02<00:00, 216.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a DPO trainer\n",
    "# This trainer will handle the fine-tuning of the model using the DPO technique\n",
    "trainer = DPOTrainer(\n",
    "        # The model to be fine-tuned\n",
    "        model=model,\n",
    "        # The reference model (not used in this case because LoRA has been used)\n",
    "        ref_model=None,\n",
    "        # The DPO training configuration\n",
    "        args=training_args,\n",
    "        # The training dataset\n",
    "        train_dataset=train_dataset,\n",
    "        # The evaluation dataset\n",
    "        eval_dataset=eval_dataset,\n",
    "        # The tokenizer for the model\n",
    "        tokenizer=tokenizer,\n",
    "        # The PEFT (Parallel Efficient Finetuning) configuration\n",
    "        peft_config=peft_lora_config,  ##lora_configuration\n",
    "        # The maximum prompt length\n",
    "        #max_prompt_length=512,\n",
    "        # The maximum sequence length\n",
    "        #max_length=512,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca6f6c-9e39-48d4-8efb-aaf3e49150a7",
   "metadata": {},
   "source": [
    "#### we observe the model already has the LoRA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ed6cfc-82ce-471d-86ca-30ad43add86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): lora.Linear(\n",
       "            (base_layer): Conv1D(nf=2304, nx=768)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (c_proj): lora.Linear(\n",
       "            (base_layer): Conv1D(nf=768, nx=768)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): lora.Linear(\n",
       "            (base_layer): Conv1D(nf=768, nx=3072)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b8465-69a0-4c83-ba0f-f76c500f93a2",
   "metadata": {},
   "source": [
    "#### training the model (fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b243590-f24b-4da8-a71e-021baa70849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 4:17:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.875300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.728800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.788600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.800300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.676900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/adrian/Documents/ibm-certifications/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.4280859022140503, metrics={'train_runtime': 15475.9211, 'train_samples_per_second': 0.323, 'train_steps_per_second': 0.323, 'total_flos': 0.0, 'train_loss': 0.4280859022140503, 'epoch': 10.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95ec45f7-5b52-4251-a1a0-9aad87d14bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a64c686a-5a23-481b-8205-7d5793d8583d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>rewards/chosen</th>\n",
       "      <th>rewards/rejected</th>\n",
       "      <th>rewards/accuracies</th>\n",
       "      <th>rewards/margins</th>\n",
       "      <th>logps/rejected</th>\n",
       "      <th>logps/chosen</th>\n",
       "      <th>logits/rejected</th>\n",
       "      <th>logits/chosen</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7008</td>\n",
       "      <td>3.060678</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.361794</td>\n",
       "      <td>0.304932</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.056862</td>\n",
       "      <td>-413.076874</td>\n",
       "      <td>-538.286621</td>\n",
       "      <td>-101.656624</td>\n",
       "      <td>-102.460472</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8753</td>\n",
       "      <td>6.097466</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>1.143791</td>\n",
       "      <td>0.861896</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.281895</td>\n",
       "      <td>-487.521576</td>\n",
       "      <td>-580.108337</td>\n",
       "      <td>-103.957932</td>\n",
       "      <td>-102.844589</td>\n",
       "      <td>0.4</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9056</td>\n",
       "      <td>6.806728</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.809113</td>\n",
       "      <td>0.858866</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.049753</td>\n",
       "      <td>-530.684082</td>\n",
       "      <td>-542.840637</td>\n",
       "      <td>-103.043922</td>\n",
       "      <td>-102.728691</td>\n",
       "      <td>0.6</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7288</td>\n",
       "      <td>9.384877</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.968836</td>\n",
       "      <td>0.793535</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.175301</td>\n",
       "      <td>-493.781677</td>\n",
       "      <td>-552.282166</td>\n",
       "      <td>-105.499397</td>\n",
       "      <td>-104.786774</td>\n",
       "      <td>0.8</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7476</td>\n",
       "      <td>34.988960</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>1.532537</td>\n",
       "      <td>0.907743</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.624793</td>\n",
       "      <td>-412.471954</td>\n",
       "      <td>-488.663940</td>\n",
       "      <td>-104.482361</td>\n",
       "      <td>-103.356308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.1167</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>1.862740</td>\n",
       "      <td>1.188733</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.674007</td>\n",
       "      <td>-489.671844</td>\n",
       "      <td>-515.749573</td>\n",
       "      <td>-104.295891</td>\n",
       "      <td>-103.433517</td>\n",
       "      <td>1.2</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5851</td>\n",
       "      <td>14.250435</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.949784</td>\n",
       "      <td>-0.248562</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.198346</td>\n",
       "      <td>-494.070190</td>\n",
       "      <td>-587.883545</td>\n",
       "      <td>-106.013046</td>\n",
       "      <td>-104.375023</td>\n",
       "      <td>1.4</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7886</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.791502</td>\n",
       "      <td>-0.974953</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.766456</td>\n",
       "      <td>-501.901886</td>\n",
       "      <td>-637.785034</td>\n",
       "      <td>-104.930603</td>\n",
       "      <td>-104.545547</td>\n",
       "      <td>1.6</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8003</td>\n",
       "      <td>0.163251</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.569785</td>\n",
       "      <td>-0.879283</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.449068</td>\n",
       "      <td>-489.634766</td>\n",
       "      <td>-499.282532</td>\n",
       "      <td>-107.401291</td>\n",
       "      <td>-107.734001</td>\n",
       "      <td>1.8</td>\n",
       "      <td>900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.7361</td>\n",
       "      <td>49.338306</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.256929</td>\n",
       "      <td>-0.969131</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.226060</td>\n",
       "      <td>-418.359222</td>\n",
       "      <td>-465.334442</td>\n",
       "      <td>-109.138496</td>\n",
       "      <td>-109.586533</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.6769</td>\n",
       "      <td>16.233465</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.927480</td>\n",
       "      <td>-1.529301</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.456781</td>\n",
       "      <td>-475.851013</td>\n",
       "      <td>-481.531616</td>\n",
       "      <td>-109.135033</td>\n",
       "      <td>-109.534241</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.8896</td>\n",
       "      <td>28.932322</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-1.306491</td>\n",
       "      <td>-3.269685</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.963194</td>\n",
       "      <td>-529.626648</td>\n",
       "      <td>-554.401611</td>\n",
       "      <td>-112.054825</td>\n",
       "      <td>-112.834122</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.6533</td>\n",
       "      <td>23.245384</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.982497</td>\n",
       "      <td>-3.189353</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.206856</td>\n",
       "      <td>-543.268311</td>\n",
       "      <td>-586.043579</td>\n",
       "      <td>-112.577820</td>\n",
       "      <td>-111.301117</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.3917</td>\n",
       "      <td>5.074166</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.111306</td>\n",
       "      <td>-2.463526</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.352220</td>\n",
       "      <td>-466.329132</td>\n",
       "      <td>-541.965637</td>\n",
       "      <td>-109.277725</td>\n",
       "      <td>-108.866608</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0005</td>\n",
       "      <td>71.109840</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.715488</td>\n",
       "      <td>-2.227286</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.511799</td>\n",
       "      <td>-486.522430</td>\n",
       "      <td>-608.283081</td>\n",
       "      <td>-109.063202</td>\n",
       "      <td>-110.624489</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.4888</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.070604</td>\n",
       "      <td>-2.582192</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.511588</td>\n",
       "      <td>-487.716095</td>\n",
       "      <td>-578.578613</td>\n",
       "      <td>-109.141396</td>\n",
       "      <td>-110.501862</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5426</td>\n",
       "      <td>27.330206</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.528470</td>\n",
       "      <td>-3.860412</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3.331943</td>\n",
       "      <td>-503.029877</td>\n",
       "      <td>-558.345093</td>\n",
       "      <td>-112.307060</td>\n",
       "      <td>-110.859901</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.191536</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>-0.583966</td>\n",
       "      <td>-4.012543</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3.428577</td>\n",
       "      <td>-520.797119</td>\n",
       "      <td>-576.349121</td>\n",
       "      <td>-108.953751</td>\n",
       "      <td>-108.424210</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.3956</td>\n",
       "      <td>13.245173</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.870325</td>\n",
       "      <td>-3.549013</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.678688</td>\n",
       "      <td>-502.270264</td>\n",
       "      <td>-529.731506</td>\n",
       "      <td>-108.954933</td>\n",
       "      <td>-108.828568</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.4304</td>\n",
       "      <td>6.509144</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-1.531459</td>\n",
       "      <td>-4.231367</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2.699908</td>\n",
       "      <td>-543.347717</td>\n",
       "      <td>-543.186462</td>\n",
       "      <td>-110.585312</td>\n",
       "      <td>-110.954002</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2509</td>\n",
       "      <td>11.059148</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-2.022980</td>\n",
       "      <td>-5.667227</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3.644246</td>\n",
       "      <td>-495.695679</td>\n",
       "      <td>-511.566498</td>\n",
       "      <td>-112.560204</td>\n",
       "      <td>-111.521484</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.4738</td>\n",
       "      <td>5.874969</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-1.726909</td>\n",
       "      <td>-5.193481</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.466571</td>\n",
       "      <td>-525.356689</td>\n",
       "      <td>-559.726929</td>\n",
       "      <td>-112.316505</td>\n",
       "      <td>-111.073753</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.2951</td>\n",
       "      <td>0.081602</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-1.739048</td>\n",
       "      <td>-6.001891</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4.262842</td>\n",
       "      <td>-559.299988</td>\n",
       "      <td>-546.863342</td>\n",
       "      <td>-108.488480</td>\n",
       "      <td>-109.418694</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.6512</td>\n",
       "      <td>4.580474</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.619926</td>\n",
       "      <td>-4.254910</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.634983</td>\n",
       "      <td>-499.505005</td>\n",
       "      <td>-548.473022</td>\n",
       "      <td>-107.170769</td>\n",
       "      <td>-108.077675</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.3859</td>\n",
       "      <td>0.031184</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.988608</td>\n",
       "      <td>-4.653471</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.664863</td>\n",
       "      <td>-552.658325</td>\n",
       "      <td>-654.687439</td>\n",
       "      <td>-107.276711</td>\n",
       "      <td>-106.854782</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.103908</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.960099</td>\n",
       "      <td>-5.916081</td>\n",
       "      <td>0.98</td>\n",
       "      <td>4.955982</td>\n",
       "      <td>-554.763367</td>\n",
       "      <td>-596.597412</td>\n",
       "      <td>-109.543121</td>\n",
       "      <td>-107.238251</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.3518</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.878065</td>\n",
       "      <td>-5.306590</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4.428524</td>\n",
       "      <td>-502.350891</td>\n",
       "      <td>-595.555786</td>\n",
       "      <td>-109.629814</td>\n",
       "      <td>-110.346329</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-1.859583</td>\n",
       "      <td>-5.839107</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.979524</td>\n",
       "      <td>-500.756714</td>\n",
       "      <td>-528.099915</td>\n",
       "      <td>-106.923561</td>\n",
       "      <td>-106.645660</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1890</td>\n",
       "      <td>2.500848</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-1.426349</td>\n",
       "      <td>-5.486184</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.059836</td>\n",
       "      <td>-506.734222</td>\n",
       "      <td>-548.261414</td>\n",
       "      <td>-106.084496</td>\n",
       "      <td>-106.490753</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-2.393196</td>\n",
       "      <td>-7.425562</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.032366</td>\n",
       "      <td>-609.935913</td>\n",
       "      <td>-557.000977</td>\n",
       "      <td>-108.661346</td>\n",
       "      <td>-108.048264</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.038952</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-1.057996</td>\n",
       "      <td>-6.103258</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5.045262</td>\n",
       "      <td>-536.386658</td>\n",
       "      <td>-522.083923</td>\n",
       "      <td>-106.688049</td>\n",
       "      <td>-106.845116</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.4230</td>\n",
       "      <td>6.472532</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-1.950643</td>\n",
       "      <td>-6.529451</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.578808</td>\n",
       "      <td>-550.455017</td>\n",
       "      <td>-629.039246</td>\n",
       "      <td>-108.620720</td>\n",
       "      <td>-109.976036</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-1.611159</td>\n",
       "      <td>-6.296078</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4.684919</td>\n",
       "      <td>-481.760223</td>\n",
       "      <td>-513.754517</td>\n",
       "      <td>-105.358917</td>\n",
       "      <td>-104.301445</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.460133</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-1.661100</td>\n",
       "      <td>-6.843077</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.181978</td>\n",
       "      <td>-548.192627</td>\n",
       "      <td>-605.910522</td>\n",
       "      <td>-109.621330</td>\n",
       "      <td>-107.880859</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0915</td>\n",
       "      <td>5.090396</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-2.030910</td>\n",
       "      <td>-7.779229</td>\n",
       "      <td>0.97</td>\n",
       "      <td>5.748319</td>\n",
       "      <td>-593.522339</td>\n",
       "      <td>-562.672241</td>\n",
       "      <td>-102.361717</td>\n",
       "      <td>-101.955383</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.3323</td>\n",
       "      <td>0.085747</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-2.873695</td>\n",
       "      <td>-8.240721</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.367026</td>\n",
       "      <td>-526.315613</td>\n",
       "      <td>-547.362183</td>\n",
       "      <td>-106.094055</td>\n",
       "      <td>-106.081604</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.519794</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-2.718457</td>\n",
       "      <td>-8.485644</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.767187</td>\n",
       "      <td>-570.330261</td>\n",
       "      <td>-577.238892</td>\n",
       "      <td>-106.084602</td>\n",
       "      <td>-106.154884</td>\n",
       "      <td>7.4</td>\n",
       "      <td>3700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.363619</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-2.872041</td>\n",
       "      <td>-8.857794</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5.985753</td>\n",
       "      <td>-527.718262</td>\n",
       "      <td>-550.283997</td>\n",
       "      <td>-108.858711</td>\n",
       "      <td>-107.540482</td>\n",
       "      <td>7.6</td>\n",
       "      <td>3800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-1.432502</td>\n",
       "      <td>-7.145418</td>\n",
       "      <td>0.93</td>\n",
       "      <td>5.712916</td>\n",
       "      <td>-583.980408</td>\n",
       "      <td>-611.396606</td>\n",
       "      <td>-106.390022</td>\n",
       "      <td>-106.893661</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.2480</td>\n",
       "      <td>1.568094</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-1.792853</td>\n",
       "      <td>-7.359137</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.566284</td>\n",
       "      <td>-567.348511</td>\n",
       "      <td>-580.956299</td>\n",
       "      <td>-105.162148</td>\n",
       "      <td>-103.768097</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.056044</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-1.403589</td>\n",
       "      <td>-7.122951</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.719361</td>\n",
       "      <td>-502.920868</td>\n",
       "      <td>-574.332397</td>\n",
       "      <td>-103.828743</td>\n",
       "      <td>-105.556702</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.376197</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-2.140582</td>\n",
       "      <td>-7.861362</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.720779</td>\n",
       "      <td>-558.983337</td>\n",
       "      <td>-610.565491</td>\n",
       "      <td>-105.696777</td>\n",
       "      <td>-103.126961</td>\n",
       "      <td>8.4</td>\n",
       "      <td>4200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-1.452242</td>\n",
       "      <td>-8.295654</td>\n",
       "      <td>0.94</td>\n",
       "      <td>6.843412</td>\n",
       "      <td>-603.228088</td>\n",
       "      <td>-550.390381</td>\n",
       "      <td>-105.463898</td>\n",
       "      <td>-105.811790</td>\n",
       "      <td>8.6</td>\n",
       "      <td>4300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0846</td>\n",
       "      <td>1.279009</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-2.188681</td>\n",
       "      <td>-7.940462</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.751781</td>\n",
       "      <td>-556.317383</td>\n",
       "      <td>-570.837708</td>\n",
       "      <td>-105.796883</td>\n",
       "      <td>-104.811462</td>\n",
       "      <td>8.8</td>\n",
       "      <td>4400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.2270</td>\n",
       "      <td>191.604263</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-2.255416</td>\n",
       "      <td>-7.712950</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.457533</td>\n",
       "      <td>-542.690002</td>\n",
       "      <td>-538.621704</td>\n",
       "      <td>-106.149628</td>\n",
       "      <td>-105.510651</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.1169</td>\n",
       "      <td>0.090371</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-2.693729</td>\n",
       "      <td>-8.639226</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.945498</td>\n",
       "      <td>-550.678955</td>\n",
       "      <td>-540.801392</td>\n",
       "      <td>-105.929901</td>\n",
       "      <td>-106.507988</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.2009</td>\n",
       "      <td>114.870094</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-2.733451</td>\n",
       "      <td>-8.765038</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6.031586</td>\n",
       "      <td>-531.872986</td>\n",
       "      <td>-513.203369</td>\n",
       "      <td>-106.142799</td>\n",
       "      <td>-105.523293</td>\n",
       "      <td>9.4</td>\n",
       "      <td>4700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.075598</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-2.197861</td>\n",
       "      <td>-7.805428</td>\n",
       "      <td>0.93</td>\n",
       "      <td>5.607568</td>\n",
       "      <td>-496.662781</td>\n",
       "      <td>-563.763184</td>\n",
       "      <td>-106.215691</td>\n",
       "      <td>-105.961639</td>\n",
       "      <td>9.6</td>\n",
       "      <td>4800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.069292</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-2.107142</td>\n",
       "      <td>-9.085177</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6.978034</td>\n",
       "      <td>-648.791992</td>\n",
       "      <td>-653.024170</td>\n",
       "      <td>-105.994171</td>\n",
       "      <td>-105.758087</td>\n",
       "      <td>9.8</td>\n",
       "      <td>4900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.1682</td>\n",
       "      <td>0.070591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.143047</td>\n",
       "      <td>-8.705962</td>\n",
       "      <td>0.97</td>\n",
       "      <td>6.562916</td>\n",
       "      <td>-576.807556</td>\n",
       "      <td>-598.302612</td>\n",
       "      <td>-106.067902</td>\n",
       "      <td>-104.218498</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>15475.9211</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss   grad_norm  learning_rate  rewards/chosen  rewards/rejected  \\\n",
       "0   0.7008    3.060678       0.000098        0.361794          0.304932   \n",
       "1   0.8753    6.097466       0.000096        1.143791          0.861896   \n",
       "2   0.9056    6.806728       0.000094        0.809113          0.858866   \n",
       "3   0.7288    9.384877       0.000092        0.968836          0.793535   \n",
       "4   0.7476   34.988960       0.000090        1.532537          0.907743   \n",
       "5   1.1167    0.008399       0.000088        1.862740          1.188733   \n",
       "6   0.5851   14.250435       0.000086        0.949784         -0.248562   \n",
       "7   0.7886    0.003695       0.000084        0.791502         -0.974953   \n",
       "8   0.8003    0.163251       0.000082        0.569785         -0.879283   \n",
       "9   0.7361   49.338306       0.000080        0.256929         -0.969131   \n",
       "10  0.6769   16.233465       0.000078        0.927480         -1.529301   \n",
       "11  0.8896   28.932322       0.000076       -1.306491         -3.269685   \n",
       "12  0.6533   23.245384       0.000074       -0.982497         -3.189353   \n",
       "13  0.3917    5.074166       0.000072       -0.111306         -2.463526   \n",
       "14  1.0005   71.109840       0.000070       -0.715488         -2.227286   \n",
       "15  0.4888    0.016467       0.000068       -0.070604         -2.582192   \n",
       "16  0.5426   27.330206       0.000066       -0.528470         -3.860412   \n",
       "17  0.5955    0.191536       0.000064       -0.583966         -4.012543   \n",
       "18  0.3956   13.245173       0.000062       -0.870325         -3.549013   \n",
       "19  0.4304    6.509144       0.000060       -1.531459         -4.231367   \n",
       "20  0.2509   11.059148       0.000058       -2.022980         -5.667227   \n",
       "21  0.4738    5.874969       0.000056       -1.726909         -5.193481   \n",
       "22  0.2951    0.081602       0.000054       -1.739048         -6.001891   \n",
       "23  0.6512    4.580474       0.000052       -0.619926         -4.254910   \n",
       "24  0.3859    0.031184       0.000050       -0.988608         -4.653471   \n",
       "25  0.1213    0.103908       0.000048       -0.960099         -5.916081   \n",
       "26  0.3518    0.004417       0.000046       -0.878065         -5.306590   \n",
       "27  0.5256    0.002146       0.000044       -1.859583         -5.839107   \n",
       "28  0.1890    2.500848       0.000042       -1.426349         -5.486184   \n",
       "29  0.4089    0.003696       0.000040       -2.393196         -7.425562   \n",
       "30  0.2929    0.038952       0.000038       -1.057996         -6.103258   \n",
       "31  0.4230    6.472532       0.000036       -1.950643         -6.529451   \n",
       "32  0.2144    0.000403       0.000034       -1.611159         -6.296078   \n",
       "33  0.1544    0.460133       0.000032       -1.661100         -6.843077   \n",
       "34  0.0915    5.090396       0.000030       -2.030910         -7.779229   \n",
       "35  0.3323    0.085747       0.000028       -2.873695         -8.240721   \n",
       "36  0.1548    0.519794       0.000026       -2.718457         -8.485644   \n",
       "37  0.1000    0.363619       0.000024       -2.872041         -8.857794   \n",
       "38  0.2301    0.032992       0.000022       -1.432502         -7.145418   \n",
       "39  0.2480    1.568094       0.000020       -1.792853         -7.359137   \n",
       "40  0.1291    0.056044       0.000018       -1.403589         -7.122951   \n",
       "41  0.1023    0.376197       0.000016       -2.140582         -7.861362   \n",
       "42  0.2083    0.023196       0.000014       -1.452242         -8.295654   \n",
       "43  0.0846    1.279009       0.000012       -2.188681         -7.940462   \n",
       "44  0.2270  191.604263       0.000010       -2.255416         -7.712950   \n",
       "45  0.1169    0.090371       0.000008       -2.693729         -8.639226   \n",
       "46  0.2009  114.870094       0.000006       -2.733451         -8.765038   \n",
       "47  0.1621    0.075598       0.000004       -2.197861         -7.805428   \n",
       "48  0.0604    0.069292       0.000002       -2.107142         -9.085177   \n",
       "49  0.1682    0.070591       0.000000       -2.143047         -8.705962   \n",
       "50     NaN         NaN            NaN             NaN               NaN   \n",
       "\n",
       "    rewards/accuracies  rewards/margins  logps/rejected  logps/chosen  \\\n",
       "0                 0.55         0.056862     -413.076874   -538.286621   \n",
       "1                 0.54         0.281895     -487.521576   -580.108337   \n",
       "2                 0.58        -0.049753     -530.684082   -542.840637   \n",
       "3                 0.57         0.175301     -493.781677   -552.282166   \n",
       "4                 0.60         0.624793     -412.471954   -488.663940   \n",
       "5                 0.74         0.674007     -489.671844   -515.749573   \n",
       "6                 0.75         1.198346     -494.070190   -587.883545   \n",
       "7                 0.73         1.766456     -501.901886   -637.785034   \n",
       "8                 0.70         1.449068     -489.634766   -499.282532   \n",
       "9                 0.67         1.226060     -418.359222   -465.334442   \n",
       "10                0.81         2.456781     -475.851013   -481.531616   \n",
       "11                0.76         1.963194     -529.626648   -554.401611   \n",
       "12                0.74         2.206856     -543.268311   -586.043579   \n",
       "13                0.83         2.352220     -466.329132   -541.965637   \n",
       "14                0.73         1.511799     -486.522430   -608.283081   \n",
       "15                0.90         2.511588     -487.716095   -578.578613   \n",
       "16                0.84         3.331943     -503.029877   -558.345093   \n",
       "17                0.85         3.428577     -520.797119   -576.349121   \n",
       "18                0.83         2.678688     -502.270264   -529.731506   \n",
       "19                0.82         2.699908     -543.347717   -543.186462   \n",
       "20                0.90         3.644246     -495.695679   -511.566498   \n",
       "21                0.89         3.466571     -525.356689   -559.726929   \n",
       "22                0.91         4.262842     -559.299988   -546.863342   \n",
       "23                0.88         3.634983     -499.505005   -548.473022   \n",
       "24                0.88         3.664863     -552.658325   -654.687439   \n",
       "25                0.98         4.955982     -554.763367   -596.597412   \n",
       "26                0.91         4.428524     -502.350891   -595.555786   \n",
       "27                0.86         3.979524     -500.756714   -528.099915   \n",
       "28                0.94         4.059836     -506.734222   -548.261414   \n",
       "29                0.88         5.032366     -609.935913   -557.000977   \n",
       "30                0.92         5.045262     -536.386658   -522.083923   \n",
       "31                0.94         4.578808     -550.455017   -629.039246   \n",
       "32                0.93         4.684919     -481.760223   -513.754517   \n",
       "33                0.94         5.181978     -548.192627   -605.910522   \n",
       "34                0.97         5.748319     -593.522339   -562.672241   \n",
       "35                0.94         5.367026     -526.315613   -547.362183   \n",
       "36                0.94         5.767187     -570.330261   -577.238892   \n",
       "37                0.92         5.985753     -527.718262   -550.283997   \n",
       "38                0.93         5.712916     -583.980408   -611.396606   \n",
       "39                0.94         5.566284     -567.348511   -580.956299   \n",
       "40                0.95         5.719361     -502.920868   -574.332397   \n",
       "41                0.95         5.720779     -558.983337   -610.565491   \n",
       "42                0.94         6.843412     -603.228088   -550.390381   \n",
       "43                0.96         5.751781     -556.317383   -570.837708   \n",
       "44                0.95         5.457533     -542.690002   -538.621704   \n",
       "45                0.94         5.945498     -550.678955   -540.801392   \n",
       "46                0.95         6.031586     -531.872986   -513.203369   \n",
       "47                0.93         5.607568     -496.662781   -563.763184   \n",
       "48                0.95         6.978034     -648.791992   -653.024170   \n",
       "49                0.97         6.562916     -576.807556   -598.302612   \n",
       "50                 NaN              NaN             NaN           NaN   \n",
       "\n",
       "    logits/rejected  logits/chosen  epoch  step  train_runtime  \\\n",
       "0       -101.656624    -102.460472    0.2   100            NaN   \n",
       "1       -103.957932    -102.844589    0.4   200            NaN   \n",
       "2       -103.043922    -102.728691    0.6   300            NaN   \n",
       "3       -105.499397    -104.786774    0.8   400            NaN   \n",
       "4       -104.482361    -103.356308    1.0   500            NaN   \n",
       "5       -104.295891    -103.433517    1.2   600            NaN   \n",
       "6       -106.013046    -104.375023    1.4   700            NaN   \n",
       "7       -104.930603    -104.545547    1.6   800            NaN   \n",
       "8       -107.401291    -107.734001    1.8   900            NaN   \n",
       "9       -109.138496    -109.586533    2.0  1000            NaN   \n",
       "10      -109.135033    -109.534241    2.2  1100            NaN   \n",
       "11      -112.054825    -112.834122    2.4  1200            NaN   \n",
       "12      -112.577820    -111.301117    2.6  1300            NaN   \n",
       "13      -109.277725    -108.866608    2.8  1400            NaN   \n",
       "14      -109.063202    -110.624489    3.0  1500            NaN   \n",
       "15      -109.141396    -110.501862    3.2  1600            NaN   \n",
       "16      -112.307060    -110.859901    3.4  1700            NaN   \n",
       "17      -108.953751    -108.424210    3.6  1800            NaN   \n",
       "18      -108.954933    -108.828568    3.8  1900            NaN   \n",
       "19      -110.585312    -110.954002    4.0  2000            NaN   \n",
       "20      -112.560204    -111.521484    4.2  2100            NaN   \n",
       "21      -112.316505    -111.073753    4.4  2200            NaN   \n",
       "22      -108.488480    -109.418694    4.6  2300            NaN   \n",
       "23      -107.170769    -108.077675    4.8  2400            NaN   \n",
       "24      -107.276711    -106.854782    5.0  2500            NaN   \n",
       "25      -109.543121    -107.238251    5.2  2600            NaN   \n",
       "26      -109.629814    -110.346329    5.4  2700            NaN   \n",
       "27      -106.923561    -106.645660    5.6  2800            NaN   \n",
       "28      -106.084496    -106.490753    5.8  2900            NaN   \n",
       "29      -108.661346    -108.048264    6.0  3000            NaN   \n",
       "30      -106.688049    -106.845116    6.2  3100            NaN   \n",
       "31      -108.620720    -109.976036    6.4  3200            NaN   \n",
       "32      -105.358917    -104.301445    6.6  3300            NaN   \n",
       "33      -109.621330    -107.880859    6.8  3400            NaN   \n",
       "34      -102.361717    -101.955383    7.0  3500            NaN   \n",
       "35      -106.094055    -106.081604    7.2  3600            NaN   \n",
       "36      -106.084602    -106.154884    7.4  3700            NaN   \n",
       "37      -108.858711    -107.540482    7.6  3800            NaN   \n",
       "38      -106.390022    -106.893661    7.8  3900            NaN   \n",
       "39      -105.162148    -103.768097    8.0  4000            NaN   \n",
       "40      -103.828743    -105.556702    8.2  4100            NaN   \n",
       "41      -105.696777    -103.126961    8.4  4200            NaN   \n",
       "42      -105.463898    -105.811790    8.6  4300            NaN   \n",
       "43      -105.796883    -104.811462    8.8  4400            NaN   \n",
       "44      -106.149628    -105.510651    9.0  4500            NaN   \n",
       "45      -105.929901    -106.507988    9.2  4600            NaN   \n",
       "46      -106.142799    -105.523293    9.4  4700            NaN   \n",
       "47      -106.215691    -105.961639    9.6  4800            NaN   \n",
       "48      -105.994171    -105.758087    9.8  4900            NaN   \n",
       "49      -106.067902    -104.218498   10.0  5000            NaN   \n",
       "50              NaN            NaN   10.0  5000     15475.9211   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second  total_flos  train_loss  \n",
       "0                        NaN                     NaN         NaN         NaN  \n",
       "1                        NaN                     NaN         NaN         NaN  \n",
       "2                        NaN                     NaN         NaN         NaN  \n",
       "3                        NaN                     NaN         NaN         NaN  \n",
       "4                        NaN                     NaN         NaN         NaN  \n",
       "5                        NaN                     NaN         NaN         NaN  \n",
       "6                        NaN                     NaN         NaN         NaN  \n",
       "7                        NaN                     NaN         NaN         NaN  \n",
       "8                        NaN                     NaN         NaN         NaN  \n",
       "9                        NaN                     NaN         NaN         NaN  \n",
       "10                       NaN                     NaN         NaN         NaN  \n",
       "11                       NaN                     NaN         NaN         NaN  \n",
       "12                       NaN                     NaN         NaN         NaN  \n",
       "13                       NaN                     NaN         NaN         NaN  \n",
       "14                       NaN                     NaN         NaN         NaN  \n",
       "15                       NaN                     NaN         NaN         NaN  \n",
       "16                       NaN                     NaN         NaN         NaN  \n",
       "17                       NaN                     NaN         NaN         NaN  \n",
       "18                       NaN                     NaN         NaN         NaN  \n",
       "19                       NaN                     NaN         NaN         NaN  \n",
       "20                       NaN                     NaN         NaN         NaN  \n",
       "21                       NaN                     NaN         NaN         NaN  \n",
       "22                       NaN                     NaN         NaN         NaN  \n",
       "23                       NaN                     NaN         NaN         NaN  \n",
       "24                       NaN                     NaN         NaN         NaN  \n",
       "25                       NaN                     NaN         NaN         NaN  \n",
       "26                       NaN                     NaN         NaN         NaN  \n",
       "27                       NaN                     NaN         NaN         NaN  \n",
       "28                       NaN                     NaN         NaN         NaN  \n",
       "29                       NaN                     NaN         NaN         NaN  \n",
       "30                       NaN                     NaN         NaN         NaN  \n",
       "31                       NaN                     NaN         NaN         NaN  \n",
       "32                       NaN                     NaN         NaN         NaN  \n",
       "33                       NaN                     NaN         NaN         NaN  \n",
       "34                       NaN                     NaN         NaN         NaN  \n",
       "35                       NaN                     NaN         NaN         NaN  \n",
       "36                       NaN                     NaN         NaN         NaN  \n",
       "37                       NaN                     NaN         NaN         NaN  \n",
       "38                       NaN                     NaN         NaN         NaN  \n",
       "39                       NaN                     NaN         NaN         NaN  \n",
       "40                       NaN                     NaN         NaN         NaN  \n",
       "41                       NaN                     NaN         NaN         NaN  \n",
       "42                       NaN                     NaN         NaN         NaN  \n",
       "43                       NaN                     NaN         NaN         NaN  \n",
       "44                       NaN                     NaN         NaN         NaN  \n",
       "45                       NaN                     NaN         NaN         NaN  \n",
       "46                       NaN                     NaN         NaN         NaN  \n",
       "47                       NaN                     NaN         NaN         NaN  \n",
       "48                       NaN                     NaN         NaN         NaN  \n",
       "49                       NaN                     NaN         NaN         NaN  \n",
       "50                     0.323                   0.323         0.0    0.428086  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "296fea49-2e2a-490d-a1a1-7716fcd26371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = pd.DataFrame(trainer.state.log_history)\n",
    "log_t = log[log['loss'].notna()]\n",
    "#log_e = log[log['eval_loss'].notna()]\n",
    "\n",
    "# Plot train and evaluation losses\n",
    "plt.plot(log_t[\"epoch\"], log_t[\"loss\"], label = \"train_loss\") \n",
    "#plt.plot(log_e[\"epoch\"], log_e[\"eval_loss\"], label = \"eval_loss\") \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f321d-5e37-48c8-84aa-e6626d290bda",
   "metadata": {},
   "source": [
    "## Generating Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7157546-cae2-45cf-b82f-0da81ff71932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last checkpoint of the trained DPO model just trained\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained('./dpo/checkpoint-5000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c764767b-5ec1-4029-a7cf-230b97911a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO response:\t Is a higher octane gasoline better for your car?\n",
      "\n",
      "Well, it's not necessarily a better one.\n",
      "\n",
      "A new study from the University of California, San Diego, finds that higher octane gasoline is more effective at reducing emissions than lower octane ones.\n",
      "\n",
      "The study, which\n",
      "============================\n",
      "\n",
      "GPT2 response:\t Is a higher octane gasoline better for your car?\n",
      "\n",
      "The answer is yes. The higher octane gasoline is more efficient and more fuel efficient.\n",
      "\n",
      "The higher octane gasoline is more efficient and more fuel efficient. The higher octane gasoline is more efficient and more fuel efficient. The higher\n"
     ]
    }
   ],
   "source": [
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "# Define the generation configuration for the DPO model\n",
    "# This sets the parameters for text generation\n",
    "generation_config = GenerationConfig(\n",
    "        # Use sampling to generate diverse text\n",
    "        do_sample=True,\n",
    "        # Top-k sampling parameter\n",
    "        top_k=1,\n",
    "        # Temperature parameter to control the randomness of the generated text\n",
    "        temperature=0.1,\n",
    "        # Maximum number of new tokens to generate\n",
    "        max_new_tokens=50,\n",
    "        # Use the end-of-sequence token as the padding token\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Define the input prompt for text generation\n",
    "PROMPT = \"Is a higher octane gasoline better for your car?\"\n",
    "#PROMPT = 'who are you?'\n",
    "# Encode the prompt using the tokenizer\n",
    "inputs = tokenizer(PROMPT, return_tensors='pt')\n",
    "\n",
    "# Generate text using the DPO model\n",
    "outputs = dpo_model.generate(**inputs, generation_config=generation_config)\n",
    "# Decode the generated text and print it\n",
    "print(\"DPO response:\\t\",tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print('============================')\n",
    "# Load the pre-trained GPT-2 model\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "# Generate text using the GPT-2 model\n",
    "outputs = gpt2_model.generate(**inputs, generation_config=generation_config)\n",
    "# Decode the generated text and print it\n",
    "print(\"\\nGPT2 response:\\t\",tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5e9f7-20c8-4267-9214-e96c775a543b",
   "metadata": {},
   "source": [
    "The dpo model was trained to be more 'helpful and honest', "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eac4f7-908f-40aa-8a96-9826e9b86be0",
   "metadata": {},
   "source": [
    "#### wrapping up the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "688d5ddf-442b-418f-86fc-3f94de108eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "generation_config = GenerationConfig(\n",
    "    # Use sampling to generate diverse text\n",
    "    do_sample=True,\n",
    "    # Top-k sampling parameter: controls the number of highest probability tokens to consider\n",
    "    top_k=1,\n",
    "    # Temperature parameter: controls the randomness of the generated text\n",
    "    temperature=0.1,\n",
    "    # Maximum number of new tokens to generate\n",
    "    max_new_tokens=50,  #25,\n",
    "    # Use the end-of-sequence token as the padding token\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db6a593e-064a-45f6-a29c-e44b4d83fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dpo_response(prompt):\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    # Generate text using the DPO model\n",
    "    outputs = dpo_model.generate(**inputs, generation_config=generation_config)\n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_gpt2_response(prompt):\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate text using the GPT-2 model\n",
    "    outputs = gpt2_model.generate(**inputs, generation_config=generation_config)\n",
    "    \n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8fcd7076-7110-489c-ba32-afd9eba43e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is fire?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO response:\t what is fire?\n",
      "\n",
      "Fire is a term used to describe a process that involves burning a substance or liquid. It's usually used to describe a process that involves burning a substance or liquid.\n",
      "\n",
      "The term fire is derived from the Latin fireus, meaning \"\n",
      "==================\n",
      "\n",
      "GPT-2 response:\t what is fire?\n",
      "\n",
      "The fire is the fire that is burning in the world.\n",
      "\n",
      "The fire is the fire that is burning in the world.\n",
      "\n",
      "The fire is the fire that is burning in the world.\n",
      "\n",
      "The fire is the fire that\n"
     ]
    }
   ],
   "source": [
    "PROMPT = input()\n",
    "dpo_response = generate_dpo_response(PROMPT)\n",
    "gpt2_response = generate_gpt2_response(PROMPT)\n",
    "\n",
    "# Print the responses\n",
    "print(\"DPO response:\\t\", dpo_response)\n",
    "print('==================')\n",
    "print(\"\\nGPT-2 response:\\t\", gpt2_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2820e0-e1c8-4ebd-ba46-28817edf60f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
